[comment encoding = UTF-8 /]
[module scriptRFile('http://www.project.ase.polimi.it/pipeline')]


[template public generateRPipeline(aPipeline : Pipeline)]

[file ('pipeline_'.concat(aPipeline.ID).concat('.r'), false, 'UTF-8')]

# This script will generate an executable version of the pipeline

# Structure of the program:
# - if sources are local files and path is not specified, place them in a folder named "sources" at the same level of the R file;
#	if sources are remote, if they don't require credentials they will be automatically downloaded, otherwise you need to manually
#	download them;
# - all output files will be placed in a folder named "output" at the same level of the R file.

# A pdf file will be exported in "output/[aPipeline.ID/].pdf"

# Import libraries (if some are missing, install them: install.packages("library_name") )

library(readr)

# install.packages("plyr")
# install.packages("XML")
# install.packages("jsonlite")
# install.packages("rjson")
library(plyr)
library(xml)
library(jsonlite)
library(rjson)


library(MASS)
library(class)
library(e1071)
library(randomForest)

# install.packages(ggplot2)
library(ggplot2)

# install.packages("dplyr")
# install.packages("scales")
# install.packages("reshape2")
# install.packages("tibble")
library(dplyr)
library(scales)
library(reshape2)
library(tibble)


set.seed(501)
pdf("output/[aPipeline.ID/].pdf")


## COLLECTION TASK
# Import sources as dataframes

[importSource(aPipeline.sources)/]

[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(IntegrationTask)) -> size() <> 0)]
## INTEGRATION TASK
# Join all data in a unique data frame

[joinSources(aPipeline)/]
[/if]


[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(CleaningTask)) -> size() <> 0)]
## CLEANING TASK
	[for (op : CleaningOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(CleaningTask)).oclAsType(CleaningTask).cleaningOperations)]
		[if (op.oclIsTypeOf(PredefinedCleaningOperation))]

# Predefined operation : [if (op.oclAsType(PredefinedCleaningOperation).type.toString() = 'removeNulls')] remove null values
	source <- na.omit(source)
			[elseif (op.oclAsType(PredefinedCleaningOperation).type.toString() = 'removeNegative')] remove negative values
				[for (attr : Attribute | op.inputAttributes)]
	source$[attr.name/] <- source['['/] source$[attr.name/] >= 0 ,[']'/]
				[/for]
			[elseif (op.oclAsType(PredefinedCleaningOperation).type.toString() = 'round')] round values
				[for (attr : Attribute | op.inputAttributes)]
	source$[attr.name/] <- round(source$[attr.name/])
				[/for]
			[elseif (op.oclAsType(PredefinedCleaningOperation).type.toString() = 'removeColumn')] remove column
				[for (attr : Attribute | op.inputAttributes)]
	source$[attr.name/] <- NULL
				[/for]
			[else] remove duplicates
	source <- unique(source)
			[/if]
		[else]
# TODO: User defined cleaning operation
# [op.ID/]: [op.oclAsType(UserDefinedCleaningOperation).type/]
		[/if]

	[/for]
[/if]


## ANALYSIS TASK

[for (op : AnalysisOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(AnalysisTask)).oclAsType(AnalysisTask).analysisOperations)]
	[if (op.oclIsTypeOf(DescriptiveAnalysisOperation))]
# Descriptive analysis: [doDescriptiveAnalysis(op.oclAsType(DescriptiveAnalysisOperation))/]
	[elseif (op.oclIsTypeOf(ClassificationAnalysisOperation))]
# Classification analysis: [doClassificationAnalysis(op.oclAsType(ClassificationAnalysisOperation))/]
	[elseif (op.oclIsTypeOf(ClusteringAnalysisOperation))]
# Clustering analysis: [doClusteringAnalysis(op.oclAsType(ClusteringAnalysisOperation))/]
	[else] 
# Predictive analysis: [doPredictiveAnalysis(op.oclAsType(PredictiveAnalysisOperation))/]
	[/if]
[/for]

[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(VisualizationTask)) -> size() <> 0)]

## VISUALIZATION TASK

	[for (op : VisualizationOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(VisualizationTask)).oclAsType(VisualizationTask).visualizationOperations)]
		[prepareChart(op.chart)/]
	[/for]

[/if]

## EXPORT TASK
[exportFiles(aPipeline.tasks -> select(t | t.oclIsTypeOf(ExportTask)).oclAsType(ExportTask))/]

dev.off()

rm(list = ls())
[/file]
[/template]


[template private readFile(s: Source, path: String)]

[if (s.format.toString() = 'TXT')]
[s.name.replaceAll('\x20', '')/] <- read.table("[path/]/[s.name/]")
# TODO
# The source '[s.name.replaceAll('\x20', '')/]' is a TXT file, every row could have been imported as a unique column, 
# check if you need to manually split it in the desired columns		
[elseif (s.format.toString() = 'XML')]
sourceXML <- xmlParse("[path/]/[s.name/]")
sourceXMLRoot <- xmlRoot(sourceXML)
sourceXMLTable <- xmlSApply(sourceXMLRoot, function(x) xmlSApply(x, xmlValue))
[s.name.replaceAll('\x20', '')/] <- data.frame(t(sourceXMLTable), row.names=NULL)
rm(sourceXML, sourceXMLRoot, sourceXMLTable)
[elseif (s.format.toString() = 'JSON')]
sourceJSON <- fromJSON("[path/]/[s.name/]")
# TODO: complete the path after sourceJSON$ to extract the table from the json file
#   Example:  list$resource$field : from the object 'list', extract the object 'resource', from the latter extract the object 'field'
#             The result will be a table where every row is a 'field'
#			  (In this case we have and array of 'list', or inside 'list' we have an array of 'resource', or ... array of 'field')
#             The last identifier must be an object
[s.name.replaceAll('\x20', '')/] <- sourceJSON$
rm(sourceJSON)
[else]
[s.name.replaceAll('\x20', '')/] <- read.table("[path/]/[s.name/]")
[/if]
[/template]


[template private importSource(aSources: OrderedSet(Source))]
[for (s : Source | aSources)]
	[if (not(s.serverName.oclIsUndefined() or s.portNumber.oclIsUndefined()))]
		[if (not(s.userName.oclIsUndefined() or s.password.oclIsUndefined()))]
# TODO
# The requested source requires credential, before continuing download it:
# Link: [s.serverName/][if not(s.portNumber.oclIsUndefined())]:[s.portNumber/][/if]/[s.name/]
# Credentials:
	# Username: [s.userName/]
	# Password: [s.password/]
		[else]
# Remote source: downloading file
download.file([s.serverName.concat(':').concat(s.portNumber.toString()).concat('/').concat(s.name)/], "sources/[s.name/]", mode = "w")	
		[/if]
[readFile(s, 'sources')/]
	[/if]
# Loading file [s.name/]
	[if (not(s.path.oclIsUndefined()))]
[readFile(s, s.path)/]
	[else]
[readFile(s, 'sources')/]
	[/if]

[/for]
[/template]


[template private joinSources(aPipeline: Pipeline)] 
source <-
[for (op: IntegrationOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(IntegrationTask)).oclAsType(IntegrationTask).integrationOperations)]
	merge(
[/for]
[for (op: IntegrationOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(IntegrationTask)).oclAsType(IntegrationTask).integrationOperations)]
	[if (i=1)]
		[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(1))).read.format.toString() = 'TXT' or
			aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(2))).read.format.toString() = 'TXT') ]
		# TODO check this merge with a txt file
		[/if]
		[aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(1))).read.name.replace('\x20', '')/], 
		[aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(2))).read.name.replace('\x20', '')/], 
		by.x = '[op.inputAttributes->at(1).name/]', by.y = '[op.inputAttributes->at(2).name/]')
	[else]
		, [aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(1))).read.name.replace('\x20', '')/], 
		by.x = '[op.inputAttributes->at(1).name/]', by.y = '[op.inputAttributes->at(2).name/]')
		[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(1))).read.format.toString() = 'TXT')]
		# TODO check this merge with a txt file
		[/if]
	[/if]
[/for]

[for (op: IntegrationOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(IntegrationTask)).oclAsType(IntegrationTask).integrationOperations)]
	[if (not op.outputAttribute.oclIsUndefined())]
	# Replace column names with output attributes name
	colnames(source)['['/]colnames(source) == '[op.inputAttributes->at(1).name/]'[']'/] = '[op.outputAttribute.name/]'
	[/if]

[/for]

[/template]


[template private doDescriptiveAnalysis(op: DescriptiveAnalysisOperation)]
[if (op.type = DescriptiveOperation::Boxplot)]
	[drawChart(ChartType::BoxPlot, op.inputAttributes)/]
[elseif (op.type = DescriptiveOperation::Histogram)]
	[drawChart(ChartType::Histogram, op.inputAttributes)/]
[elseif (op.type = DescriptiveOperation::PieChart)]
	[drawChart(ChartType::Pie, op.inputAttributes)/]
[elseif (op.type = DescriptiveOperation::ScatterPlot)]
	[drawChart(ChartType::Scatter, op.inputAttributes)/]
[else] view data
View(source)
source
[/if]
[/template]


[template private writeFormula(attributes: OrderedSet(Attribute))]
[for (attr : Attribute | attributes)]	
	[if (i = 1)]
		[attr.name.concat(' ~ ')/]
	[elseif (i = 2)]
		[attr.name/]
	[else]
		[' + '.concat(attr.name)/]
	[/if]			
[/for]
[/template]


[template private writeColumns(attributes: OrderedSet(Attribute), sep: String)]
[for (attr : Attribute | attributes)]	
	[if (i = 1)]
		[attr.name.concat(sep)/]
	[elseif (i = 2)]
		[', '.concat(attr.name).concat(sep)/]
	[/if]				
[/for]
[/template]


[template private doClassificationAnalysis(op: ClassificationAnalysisOperation)]
[if (op.type.toString() = 'LogisticRegression')] logistic regression
	[op.ID/] <- glm([writeFormula(op.inputAttributes)/], data = source, family = binomial)
	summary([op.ID/])
[elseif (op.type.toString() = 'LDA')] LDA
	[op.ID/] <- lda([writeFormula(op.inputAttributes)/], data = source)
	[op.ID/]
[elseif (op.type.toString() = 'QDA')] QDA
	[op.ID/] <- qda([writeFormula(op.inputAttributes)/], data = source)
	[op.ID/]
[elseif (op.type.toString() = 'KNN')] KNN
	knnInd <- sample(nrow(source), round(nrow(source) * 0.8)
	train  <- cbind([writeColumns(op.inputAttributes, '[knnInd, ]')/])
	knnInd <- -knnInd
	test   <- cbind([writeColumns(op.inputAttributes, '[knnInd, ]')/])
	# TODO: change k_KNN if you need a different k
	k_KNN <- 3 
	[op.ID/] <- knn(train, test, k = k_KNN, prob = TRUE)
	source$[op.outputAttribute.name/] <- [op.ID/]
	rm(knnInd, train, test)
[else] SVM
	[op.ID/] <- svm([op.inputAttributes.name/] ~ ., data = source, kernel = "linear")
	[op.ID/]
[/if]
[/template]


[template private doClusteringAnalysis(op: ClusteringAnalysisOperation)]
[if (op.type.toString() = 'kMeans')] kMeans
	[op.ID/] <- kmeans(source, [op.k/])
	plot(source, col = [op.ID/]$cluster)
	source$[op.outputAttribute.name/] <- [op.ID/]$cluster
[else] hierarchical clustering
	[op.ID/] <- hclust(
	[if (op.inputAttributes -> size() = 0)] source
	[else] dist(data.frame([writeColumns(op.inputAttributes, '')/]))
	[/if], [op.k/])
	source$[op.outputAttribute.name/] <- [op.ID/]$order
	plot([op.ID/])
[/if]
[/template]


[template private doPredictiveAnalysis(op: PredictiveAnalysisOperation)]
[if (op.type.toString() = 'SimpleRegression')] simple regression
	[op.ID/] <- lm([op.inputAttributes->at(1).name/] ~ [op.inputAttributes->at(2).name/], data = source)
	summary([op.ID/])
[elseif (op.type.toString() = 'MultipleLinRegr')] multiple linear regression
	[op.ID/] <- lm([writeFormula(op.inputAttributes)/], data = source)
	summary([op.ID/])
[else] random forests
	[op.ID/] <- randomForest([op.inputAttributes.name/], data = source, importante = TRUE, proximity = TRUE)
	print([op.ID/])
[/if]
[/template]


[template private prepareChart(chart: Chart)]
[drawChart(chart.type, chart.axes)/]
[/template]


[template private drawChart(type: ChartType, attributes: OrderedSet(Attribute))]

data2plot <- data.frame(	
[for (attr: Attribute | attributes)]
	[if (i > 1)] , [/if]
	[attr.name/] = source$[attr.name/]
[/for]
	row.names = FALSE)
	
[if (type.toString() = 'Area')]
	ggplot(data2plot, aes(x = [attributes->at(1).name/] [if (attributes -> size() > 1)], fill=[attributes->at(2).name/] [/if])) + geom_area(aes(y = ..density..), stat = "bin")
[elseif (type.toString() = 'Bar')]
	[if (attributes -> size() = 1)] 
	ggplot(data2plot, aes(x = factor([attributes->at(1).name/]))) + geom_bar(stat = "bin") + coord_flip()
	[else]
	ggplot(data2plot, aes(x = [attributes->at(1).name/], y = [attributes->at(2).name/])) + geom_bar(stat ="identity") + coord_flip()
	[/if]
[elseif (type.toString() = 'BoxPlot')]
	data2plot['['/] , 1[']'/] <- as.factor(data2plot['['/] , 1[']'/]) 
	ggplot(data2plot, aes(x = [attributes->at(1).name/], y = [attributes->at(2).name/])) + geom_boxplot()	
[elseif (type.toString() = 'Column')]
	[if (attributes -> size() = 1)] 
	ggplot(data2plot, aes(x = factor([attributes->at(1).name/]))) + geom_bar(stat = "bin")
	[else]
	ggplot(data2plot, aes(x = [attributes->at(1).name/], y = [attributes->at(2).name/])) + geom_bar(stat ="identity")
	[/if]
[elseif (type.toString() = 'Histogram')]
	[if (attributes -> size() = 1)] 
	ggplot(data2plot, aes(x = [attributes->at(1).name/])) + geom_histogram()
	[else]
	ggplot(data2plot, aes(x = [attributes->at(1).name/], color = [attributes->at(2).name/])) + geom_histogram()
	[/if]
[elseif (type.toString() = 'Line')]
	ggplot(data2plot, aes(x = [attributes->at(1).name/], y = [attributes->at(2).name/], group = [if (attributes -> size() <= 2)] 1 [else] [attributes->at(3).name/] [/if])) + geom_line() + geom_point()
[elseif (type.toString() = 'Pie')]
	[if (attributes -> size() > 1)] 
	ggplot(data2plot, aes(x = "", y = [attributes->at(1).name/], fill = [attributes->at(2).name/])) + geom_bar(width = 1, stat = "identity") + coord_polar("y", start = 0)
	[else]
	ggplot(data2plot, aes(x = factor(1), fill = [attributes->at(1).name/])) + geom_bar(width = 1, stat = "identity") + coord_polar("y", start = 0)
	[/if]
[elseif (type.toString() = 'Radar')]
	line_plot <- data2plot %>% filter(variable == '[attributes->at(1).name/]') %>%
 		ggplot(aes(x = [attributes->at(2).name/], y = [attributes->at(3).name/], group = 1)) + geom_line()
	print(line_plot + coord_polar())
[elseif (type.toString() = 'Scatter')]
	ggplot(data2Plot, aes(x = [attributes->at(1).name/], y = [attributes->at(2).name/] [if (attributes -> size() > 2)] , col = [attributes->at(3).name/] [/if])) + geom_point()
[/if]
[/template]


[template private exportFiles(aTask: Sequence(ExportTask))]
[for (exp : ExportOperation | aTask.exportOperations)]
	[if (exp.write.format.toString() = 'XML')]
	xml <- xmlTree()
	xml$addTag("pipeline_data", close = FALSE)
	xml$addTag("rows", close = FALSE)
	for (i in 1:nrow(source)) {
  		xml$addTag("row", close = FALSE)
  		for (j in names(source)) {
    		xml$addTag(j, source['['/]i, j[']'/])
  		}
  		xml$closeTag()
	}
	xml$closeTag()
	xml$closeTag()
	saveXML(xml, "output/[exp.write.name/]")
	[elseif (exp.write.format.toString() = 'JSON')]
	json <- toJSON(unname(split(source, 1:nrow(source))))
	write(json, "output/[exp.write.name/]")
	[else]
	write.table(source, file = "output/[exp.write.name/]")
	[/if]
[/for]
[/template]

