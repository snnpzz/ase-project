[comment encoding = UTF-8 /]
[module scriptRFile('http://www.project.ase.polimi.it/pipeline')]


[template public generateRPipeline(aPipeline : Pipeline)]

[file ('pipeline_'.concat(aPipeline.ID).concat('.r'), false, 'UTF-8')]

# This will generate an executable version of the pipeline

# Structure of the program:
# - if sources are local files and path is not specified, place them in a folder named "sources" at the same level of the R file;
#	if sources are remote, if they don't require credentials they will be automatically downloaded, otherwise you need to manually
#	download them;
# - all output files will be placed in a folder named "output" at the same level of the R file.

# A pdf file will be exported in "output/[aPipeline.ID/].pdf"

# Import libraries (if some are missing, install them: install.packages("library_name") )
library(readr)

library(MASS)
library(class)
library(e1071)
library(randomForest)

# install.packages(ggplot2)
library(ggplot2)


set.seed(501)
pdf("output/[aPipeline.ID/].pdf")


## COLLECTION TASK
# Import sources as dataframes

[importSource(aPipeline.sources)/]

[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(IntegrationTask)) -> size() <> 0)]
## INTEGRATION TASK
# Join all data in a unique data frame

[joinSources(aPipeline)/]
[/if]


[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(CleaningTask)) -> size() <> 0)]
## CLEANING TASK
	[for (op : CleaningOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(CleaningTask)).oclAsType(CleaningTask).cleaningOperations)]
		[if (op.oclIsTypeOf(PredefinedCleaningOperation))]

# Predefined operation : [if (op.oclAsType(PredefinedCleaningOperation).type.toString() = 'removeNulls')] remove null values
	source <- na.omit(source)
			[elseif (op.oclAsType(PredefinedCleaningOperation).type.toString() = 'removeNegative')] remove negative values
				[for (attr : Attribute | op.inputAttributes)]
	source$[attr.name/] <- source['['/] source$[attr.name/] >= 0 ,[']'/]
				[/for]
			[elseif (op.oclAsType(PredefinedCleaningOperation).type.toString() = 'round')] round values
				[for (attr : Attribute | op.inputAttributes)]
	source$[attr.name/] <- round(source$[attr.name/])
				[/for]
			[elseif (op.oclAsType(PredefinedCleaningOperation).type.toString() = 'removeColumn')] remove column
				[for (attr : Attribute | op.inputAttributes)]
	source$[attr.name/] <- NULL
				[/for]
			[else] remove duplicates
	source <- unique(source)
			[/if]
		[else]
# TODO: User defined cleaning operation
# [op.ID/]: [op.oclAsType(UserDefinedCleaningOperation).type/]
		[/if]

	[/for]
[/if]


## ANALYSIS TASK

[for (op : AnalysisOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(AnalysisTask)).oclAsType(AnalysisTask).analysisOperations)]
	[if (op.oclIsTypeOf(DescriptiveAnalysisOperation))]
# Descriptive analysis: [doDescriptiveAnalysis(op.oclAsType(DescriptiveAnalysisOperation))/]
	[elseif (op.oclIsTypeOf(ClassificationAnalysisOperation))]
# Classification analysis: [doClassificationAnalysis(op.oclAsType(ClassificationAnalysisOperation))/]
	[elseif (op.oclIsTypeOf(ClusteringAnalysisOperation))]
# Clustering analysis: [doClusteringAnalysis(op.oclAsType(ClusteringAnalysisOperation))/]
	[else] 
# Predictive analysis: [doPredictiveAnalysis(op.oclAsType(PredictiveAnalysisOperation))/]
	[/if]
[/for]

[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(VisualizationTask)) -> size() <> 0)]

## VISUALIZATION TASK

	[for (op : VisualizationOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(VisualizationTask)).oclAsType(VisualizationTask).visualizationOperations)]
		[prepareChart(op.chart)/]
	[/for]

[/if]

## EXPORT TASK
[exportFiles(aPipeline.tasks -> select(t | t.oclIsTypeOf(ExportTask)).oclAsType(ExportTask))/]

dev.off()

rm(list = ls())
[/file]
[/template]


[template private importSource(aSources: OrderedSet(Source))]
[for (s : Source | aSources)]
	[if (not(s.serverName.oclIsUndefined() or s.portNumber.oclIsUndefined()))]
		[if (not(s.userName.oclIsUndefined() or s.password.oclIsUndefined()))]
# TODO
# The requested source requires credential, before continuing download it:
# Link: [s.serverName/][if not(s.portNumber.oclIsUndefined())]:[s.portNumber/][/if]/[s.name/]
# Credentials:
	# Username: [s.userName/]
	# Password: [s.password/]
		[else]
# Remote source: downloading file
download.file([s.serverName.concat(':').concat(s.portNumber.toString()).concat('/').concat(s.name)/], "sources/[s.name/]", mode = "w")	
		[/if]
[s.name.replace('.', '')/] <- read.table("sources/[s.name/]")
	[/if]
# Loading file [s.name/]
	[if (not(s.path.oclIsUndefined()))]
[s.name.replaceAll('\x20', '')/] <- read.table("[s.path/]/[s.name/]")
	[else]
[s.name.replaceAll('\x20', '')/] <- read.table("sources/[s.name/]")
	[/if]
	[if (s.format.toString() = 'TXT')]
# TODO
# The source '[s.name.replaceAll('\x20', '')/]' is a TXT file, every row has been imported as a unique column, manually split it		
	[/if]

[/for]
[/template]


[template private joinSources(aPipeline: Pipeline)] 
source <-
[for (op: IntegrationOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(IntegrationTask)).oclAsType(IntegrationTask).integrationOperations)]
	merge(
[/for]
[for (op: IntegrationOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(IntegrationTask)).oclAsType(IntegrationTask).integrationOperations)]
	[if (i=1)]
		[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(1))).read.format.toString() = 'TXT' or
			aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(2))).read.format.toString() = 'TXT') ]
		# TODO check this merge with a txt file
		[/if]
		[aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(1))).read.name.replace('\x20', '')/], 
		[aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(2))).read.name.replace('\x20', '')/], 
		by.x = '[op.inputAttributes->at(1).name/]', by.y = '[op.inputAttributes->at(2).name/]')
	[else]
		, [aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(1))).read.name.replace('\x20', '')/], 
		by.x = '[op.inputAttributes->at(1).name/]', by.y = '[op.inputAttributes->at(2).name/]')
		[if (aPipeline.tasks -> select(t | t.oclIsTypeOf(CollectionTask)).oclAsType(CollectionTask).importOperations -> select(imp | imp.use.attributes -> exists(a | a = op.inputAttributes->at(1))).read.format.toString() = 'TXT')]
		# TODO check this merge with a txt file
		[/if]
	[/if]
[/for]

[for (op: IntegrationOperation | aPipeline.tasks -> select(t | t.oclIsTypeOf(IntegrationTask)).oclAsType(IntegrationTask).integrationOperations)]
	[if (not op.outputAttribute.oclIsUndefined())]
	# Replace column names with output attributes name
	colnames(source)['['/]colnames(source) == '[op.inputAttributes->at(1).name/]'[']'/] = '[op.outputAttribute.name/]'
	[/if]

[/for]

[/template]


[template private doDescriptiveAnalysis(op: DescriptiveAnalysisOperation)]
[if (op.type = DescriptiveOperation::Boxplot)]
	[drawChart(ChartType::BoxPlot, op.inputAttributes)/]
[elseif (op.type = DescriptiveOperation::Histogram)]
	[drawChart(ChartType::Histogram, op.inputAttributes)/]
[elseif (op.type = DescriptiveOperation::PieChart)]
	[drawChart(ChartType::Pie, op.inputAttributes)/]
[elseif (op.type = DescriptiveOperation::ScatterPlot)]
	[drawChart(ChartType::Scatter, op.inputAttributes)/]
[else] view data
View(source)
source
[/if]
[/template]


[template private writeFormula(attributes: OrderedSet(Attribute))]
[for (attr : Attribute | attributes)]	
	[if (i = 1)]
		[attr.name.concat(' ~ ')/]
	[elseif (i = 2)]
		[attr.name/]
	[else]
		[' + '.concat(attr.name)/]
	[/if]			
[/for]
[/template]


[template private writeColumns(attributes: OrderedSet(Attribute), sep: String)]
[for (attr : Attribute | attributes)]	
	[if (i = 1)]
		[attr.name.concat(sep)/]
	[elseif (i = 2)]
		[', '.concat(attr.name).concat(sep)/]
	[/if]				
[/for]
[/template]

[template private doClassificationAnalysis(op: ClassificationAnalysisOperation)]
[if (op.type.toString() = 'LogisticRegression')] logistic regression
	[op.ID/] <- glm([writeFormula(op.inputAttributes)/], data = source, family = binomial)
	summary([op.ID/])
[elseif (op.type.toString() = 'LDA')] LDA
	[op.ID/] <- lda([writeFormula(op.inputAttributes)/], data = source)
	[op.ID/]
[elseif (op.type.toString() = 'QDA')] QDA
	[op.ID/] <- qda([writeFormula(op.inputAttributes)/], data = source)
	[op.ID/]
[elseif (op.type.toString() = 'KNN')] KNN
	knnInd <- sample(nrow(source), round(nrow(source) * 0.8)
	train  <- cbind([writeColumns(op.inputAttributes, '[knnInd, ]')/])
	knnInd <- -knnInd
	test   <- cbind([writeColumns(op.inputAttributes, '[knnInd, ]')/])
	# TODO: change k_KNN if you need a different k
	k_KNN <- 3 
	[op.ID/] <- knn(train, test, k = k_KNN, prob = TRUE)
	source$[op.outputAttribute.name/] <- [op.ID/]
	rm(knnInd, train, test)
[else] SVM
	[op.ID/] <- svm([op.inputAttributes.name/] ~ ., data = source, kernel = "linear")
	[op.ID/]
[/if]
[/template]


[template private doClusteringAnalysis(op: ClusteringAnalysisOperation)]
[if (op.type.toString() = 'kMeans')] kMeans
	[op.ID/] <- kmeans(source, [op.k/])
	plot(source, col = [op.ID/]$cluster)
	source$[op.outputAttribute.name/] <- [op.ID/]$cluster
[else] hierarchical clustering
	[op.ID/] <- hclust(
	[if (op.inputAttributes -> size() = 0)] source
	[else] dist(data.frame([writeColumns(op.inputAttributes, '')/]))
	[/if], [op.k/])
	source$[op.outputAttribute.name/] <- [op.ID/]$order
	plot([op.ID/])
[/if]
[/template]


[template private doPredictiveAnalysis(op: PredictiveAnalysisOperation)]
[if (op.type.toString() = 'SimpleRegression')] simple regression
	[op.ID/] <- lm([op.inputAttributes->at(1).name/] ~ [op.inputAttributes->at(2).name/], data = source)
	summary([op.ID/])
[elseif (op.type.toString() = 'MultipleLinRegr')] multiple linear regression
	[op.ID/] <- lm([writeFormula(op.inputAttributes)/], data = source)
	summary([op.ID/])
[else] random forests
	[op.ID/] <- randomForest([op.inputAttributes.name/], data = source, importante = TRUE, proximity = TRUE)
	print([op.ID/])
[/if]
[/template]


[template private prepareChart(chart: Chart)]
[drawChart(chart.type, chart.axes)/]
[/template]

[comment TODO all ggplot? /]
[template private drawChart(type: ChartType, attributes: OrderedSet(Attribute))]

data2plot <- data.frame(	
[for (attr: Attribute | attributes)]
	[attr.name.concat(', ')/]
[/for]
	row.names = FALSE)
	
[if (type.toString() = 'Area')]
[elseif (type.toString() = 'Bar')]
	barplot(data2plot, horizontal = TRUE)
[elseif (type.toString() = 'BoxPlot')]
	boxplot(data2plot)
[elseif (type.toString() = 'Column')]
	barplot(data2plot)
[elseif (type.toString() = 'Histogram')]
	hist(data2plot)
[elseif (type.toString() = 'Line')]
	plot(data2plot)
	lines(data2plot['['/] , 2[']'/])
[elseif (type.toString() = 'Pie')]
[elseif (type.toString() = 'Radar')]
[elseif (type.toString() = 'Scatter')]
	plot(data2Plot)
[elseif (type.toString() = 'Surface')]
[/if]
[/template]


[template private exportFiles(aTask: Sequence(ExportTask))]
[for (exp : ExportOperation | aTask.exportOperations)]
	write.table(source, file = "output/[exp.write.name/]")
[/for]
[/template]

